{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepStream (Video Analytics Pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Deepstream applications introduce deep neural networks and other complex processing tasks into a stream processing pipeline to enable near real-time analytics on video and other sensor data. Extracting meaningful insights from these sensors creates opportunities for improved operational efficiences and safety. Cameras, for example, are the most deployed IoT sensor currently in use; cameras are found in our homes, on our streets, in parking los, shopping malls, warehouses, factories - they are everywhere. The potential use of video analytics is enormous: access control, loss prevention, automated checkout, surveillance, safety, automated inspection (QA), package sort (smart logistics), traffic control/engineering, industrial automation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ds_09.png\" style=\"width: 1080px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although intelligent video analytics (IVA) differs by industry and application, the flow from pixels to insights remains consistent across all use cases. It is this common workflow that is the foundation for the DeepStream SDK generic streaming analytics plug and play architecture.\n",
    "\n",
    "If we think about a traditional computer vision pipeline, we will most likely like to do something like capture a set of video streams and then run some AI workload on each video stream.  To do that however, we also need to do things like decode each video stream and then pre-process or batch the videos/images so that the AI model knows how to process them.  After the AI models have run, we also may want to run some tracking if we are doing detection and then grab some metadata for further analytics.  Without a software stack, each of these components need to be developed manually and work in concert with each other flawlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ds_04.png\" style=\"width: 1080px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that NVIDIA developed the DeepStream SDK which assists developers in creating entire pipelines for video analytics including the capture & decode, pre-processing, AI inference, tracking, analytics, and then composition for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ds_05.png\" style=\"width: 1080px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool things about the DeepStream SDK which makes it useful across the board is it's ability to have each block or plugin run directly on a portion of the GPU.\n",
    "\n",
    "For example, typically for decoding a video stream, there would be a need to develop a software decoder which would most likely not be optimized for performance (i.e. speed).  When using the DeepStream SDK, the configuration can utilize hardware acceleration with specialized units on the GPU itself for faster decoding.\n",
    "\n",
    "Similar things can be said about each portion of the video analytics pipeline; the AI inference, tracker, and analytics software can be run directly on the GPU or even on the Deep Learning Accelerator (DLA) or the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ds_06.png\" style=\"width: 1080px; float: center;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we think about a DeepStream application, we can think of each of the above operations as a block which is going to output some metadata or information to the next portion of the pipeline.  More specifically, a DeepStream application is a set of modular plugins connected to form a processing pipeline. Each plugin represents a functional block, e.g., inference using TensorRT, or multi-stream decode. Hardware accelerated plugins interact with the underlying hardware (where applicable) to deliver maximum performance. For example, the decode plugin interacts with NVDEC, and then inference plugin interacts with the GPU or DLA. Each plugin can be instantiated multiple times within a pipeline as needed.\n",
    "\n",
    "## DeepStream SDK\n",
    "\n",
    "The [NVIDIA DeepStream SDK](https://developer.nvidia.com/deepstream-sdk) is a streaming analytics toolkit based on the open source GStreamer multimedia framework. The DeepStream SDK accelerates development of scalable IVA applicatins, making it easier for developers to build core deep learning networks instead of designing end-to-end applications from scratch. The SDK is supported on systems that contain an NVIDIA Jetson module or an NVIDA dGPU adapter; it is comprisde of an extensible collection of hardware-accelerated plugins that interact with low-level libraries to optimize performance and defines a standardized metadata structure enabling custom/user-specific additions.\n",
    "\n",
    "The materials and descriptions in this notebook provide a quick overview, for more detailed information and explanation of the DeepStream SDK refer to the following materials:\n",
    "\n",
    "* [NVIDIA DeepStream SDK Development Guide](https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html)\n",
    "* [NVIDIA DeepStream Plugin Manual](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html)\n",
    "* [NVIDIA DeepStream SDK API Reference Documentation](https://docs.nvidia.com/metropolis/deepstream/dev-guide/DeepStream%20Development%20Guide/baggage/index.html)\n",
    "\n",
    "### Reference Applications\n",
    "\n",
    "The DeepStream SDK is packaged with several test applications including pre-trained models, example configuration files and sample video streams that can be used to run those applications. Additional samples and source code examples provide enough information to jumpstart development efforts for most IVA use cases. Test applications demonstrate:\n",
    "\n",
    "* How to use DeepStream elements (e.g., get source, decode and mux multiple streams, run inference on pre-trained models, annotate and render image)\n",
    "* How to generate a batch of frames and run inference on it for better resource utilization\n",
    "* How to add custom/user-specific metadata to any component of DeepStream\n",
    "* And much more... see the NVIDIA DeepStream SDK Development Guide for complete details\n",
    "\n",
    "### GStreamer Plugins\n",
    "\n",
    "GStreamer is a framework for plugins, data flow, and media type handling/negotiation. It is used to create streaming media applications. Plugins are shared libraries that are dynamically loaded at runtime and can be extended and upgraded independently. When arranged and linked together, plugins form the processing pipeline that defines the data flow for a streaming media application. You can learn more about GStreamer through its extensive online documentation, beginning with [\"What is GStreamer?\"](https://gstreamer.freedesktop.org/documentation/application-development/introduction/gstreamer.html?gi-language=c).\n",
    "\n",
    "In addition to the open source plugins you'll find in the GStreamer framework libraries, the DeepStream SDK includes NVIDIA hardware accelerated plugins that leverage GPU capabilities. For a complete list of DeepStream GStreamer plugins, see the [NVIDIA DeepStream Plugin Manual](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html#page/DeepStream_Plugin_Manual%2Fdeepstream_plugin_introduction.html).\n",
    "\n",
    "Some of the noteable NVIDIA Hardware Accelerated plugins include:\n",
    "\n",
    "- [Gst-nvstreammux](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.03.html) - Batch streams before sending for AI inference.\n",
    "- [Gst-nvinfer](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.01.html#wwpID0E0IZ0HA) - Run inference using TensorRT.\n",
    "- [Gst-nvvideo4linux2](https://docs.nvidia.com/metropolis/deepstream/4.0.1/plugin-manual/index.html#page/DeepStream_Plugin_Manual/deepstream_plugin_details.02.12.html) - Decode video streams using the hardware accelerated decoder (NVDEC); Encode RAW data in I420 format to H264 or H265 output video stream using hardware accelerated encoder (NVENC).\n",
    "- [Gst-nvvideoconvert](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.07.html) - Perform video color format conversion. The first Gst-nvvideoconvert plugin before Gst-nvdsosd plugin converts stream data from I420 to RGBA and the second Gst-nvvideoconvert plugin after Gst-nvdsosd plugin converts data from RGBA to I420.\n",
    "- [Gst-nvdsosd](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.06.html) - Draw bounding boxes, text, and region of interest (ROI) polygons.\n",
    "- [Gst-nvtracker](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/DeepStream_Plugin_Manual/deepstream_plugin_details.02.02.html) - Track object between frames.\n",
    "- [Gst-nvmultistreamtiler](https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html#page/DeepStream_Plugin_Manual%2Fdeepstream_plugin_details.02.05.html) - Composite a 2D tile from batched buffers.\n",
    "- [Gst-nvv4l2decoder](https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf) - Decode a video stream.\n",
    "- [Gst-Nvv4l2h264enc](https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf) - Encode a video stream.\n",
    "- [Gst-NvArgusCameraSrc](https://developer.download.nvidia.com/embedded/L4T/r32_Release_v1.0/Docs/Accelerated_GStreamer_User_Guide.pdf) - Provide options to control ISP properties using the Argus API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GStreamer (Gst)\n",
    "\n",
    "We won't go too in-depth into GStreamer in this particular lesson, but suffice it to say that DeepStream is built on top of GStreamer which is an open-source multimedia framework for building and constructing graphs of media-handling components (like video or audio).\n",
    "\n",
    "Below you can try out the `gst-launch-1.0` command with a simple help option just to see the available options to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gst-launch-1.0 --help-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be running this as part of this lesson, but to build a simple pipeline that takes in a video (`input.h264`) and then runs some sort of inference on it and then returns the output video (`output.mp4`) could be written as something like this.\n",
    "\n",
    "```bash\n",
    "gst-launch-1.0 filesrc location=input.h264 \\\n",
    "    ! h264parse \\\n",
    "    ! nvv4l2decoder \\\n",
    "    ! m.sink_0 nvstreammux name=m batch-size=1 width=1920 height=1080 \\\n",
    "    ! nvinfer config-file-path=deepstream-config.txt \\\n",
    "    ! nvvideoconvert \\\n",
    "    ! nvdsosd \\\n",
    "    ! nvvideoconvert \\\n",
    "    ! ‘video/x-raw,format=I420’ \\\n",
    "    ! avenc_mpeg4 bitrate=2000000 \\\n",
    "    ! mpeg4videoparse \\\n",
    "    ! mux.video_0 qtmux name=mux \\\n",
    "    ! filesink location=output.mp4\n",
    "```\n",
    "\n",
    "As you can see, this can get very complicated very quickly especially as the pipeline gets more and more complicated.  As more codecs and more components become involved, it's much easier to make a mistake and debugging GStreamer pipelines that are written like the previous example is sometimes a pain.\n",
    "\n",
    "For that reason, NVIDIA has created the DeepStream SDK which basically gives you the ability to create the same type of pipelines in a much easier manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# (1) Building a Pipeline with DeepStream Configuration Files (deepstream-app)\n",
    "\n",
    "One of the ways to interact with DeepStream is to create/edit a configuration tile which tells the SDK how to construct the pipeline.\n",
    "\n",
    "The Deepstream SDK comes with multiple sample configuration files that can be used out of the box for different use cases. Some of these include (these are located in the `/opt/nvidia/deepstream/deepstream-6.1/samples/configs/deepstream-app/` directory on your device where DeepStream was installed:\n",
    "\n",
    "* **config_infer_primary.txt**: Configures a nvinfer element as primary detector.\n",
    "* **config_infer_secondary_carcolor.txt**, **config_infer_secondary_carmake.txt**, **config_infer_secondary_vehicletypes.txt**: Configure a nvinfer element as secondary classifier.\n",
    "* **config_tracker_IOU.txt**: Configures a low-level IOU (Intersection over Union) tracker.\n",
    "* **config_tracker_NvDCF_accuracy.yml**: Config file for NvDCF tracker for higher accuracy.\n",
    "* **config_tracker_NvDCF_max_perf.yml**: Config file for NvDCF tracker for max perf mode.\n",
    "* **config_tracker_NvDCF_perf.yml**: Config file for NvDCF tracker for perf mode.\n",
    "* **config_tracker_DeepSORT.yml**: Config file for DeepSORT tracker.\n",
    "* **source1_usb_dec_infer_resnet_int8.txt**: Demonstrates one USB camera as input.\n",
    "* **source2_1080p_dec_infer-resnet_demux_int8.txt**: Demonstrates demux mode for two sources.\n",
    "* **source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.txt**: (4 Decode + Infer + SGIE + Tracker)\n",
    "* **source30_1080p_dec_infer-resnet_tiled_display_int8.txt**: (30 Decode + Infer)\n",
    "* **sources_30.csv**: CSV file for 30 sources required in source30_1080p_dec_infer-resnet_tiled_display_int8.yml.\n",
    "* **sources_4.csv**: CSV file for 30 sources required in source4_1080p_dec_infer-resnet_tracker_sgie_tiled_display_int8.yml.\n",
    "\n",
    "We've created a few specialty configuration files based on these default ones specifically for this workshop.  The first configuration file we want to look at is [source2_mp4.txt](configs/source2_mp4.txt).  Feel free to check out the entire configuration file, but we will investigate a few key pieces below.\n",
    "\n",
    "First, let's investigate the `source` portion of the configuration file (in this case, denoted `source0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing lines 28 --> 39 in the sourc2_mp4.txt file\n",
    "\n",
    "!sed -n 28,35p configs/source2_mp4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice with the `source` portion of the configuration file (basically the pipeline input), we can set parameters like the type of video source (cameras, URI, RTSP, etc.) as well as memory types to use during the pipeline and GPU_ID (in this workshop, we only have 1 GPU, but on other machines, you could potentially have more). For this example, notice that we are using MultiURI, but only providing 1 URI and then specifying num-sources=2. This will just replicate the same video twice (in essence, streaming 2 videos, they just have the same content).\n",
    "\n",
    "Next if we look at the `sink` portion of the configuration file, we can see a few options for streaming the output (i.e. output file, RTSP, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n 41,83p configs/source2_mp4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a few different sink types. First is the `EglSink` which will act as a on on screen display in most cases. `sink1` represents output of the pipeline being streamed to a file (in this case an .mp4 file) which is what we will be using for this example. Notice with this option you can specify which codec to use (h264 or h265) as well as wehter to use the hardware encoder as part of the device or a software encoder for the video stream. The last sink, `sink2`, represents output to an RTSP stream. Once running, this can be viewed with most video streaming players (like VLC). To access, you can simply navigate to `rtsp://<ip-address>:8554/ds-test` and the browser will ask to open VLC (or equivalent on your local machine).\n",
    "\n",
    "For the purposes of this lab, we will be using the second option (output to a file, mp4 video) so that it is viewable in Jupyter notebooks as shown in `sink1`.\n",
    "\n",
    "A few other configuration parameters that we could modify include `tiled-display` for creating the output view in tiled format, `osd` for on-screen display settings, and `streammux` which handles the multiplexing of the multiple streams.\n",
    "\n",
    "In this configuration, we also have set paths and properties for the models we wish to use for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n 122,180p configs/source2_mp4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see each of the properties designated with a `gie` (GPU Inference engine) represent one of the models that will be used in the inference pipeline. In this case, we will be running a primary object detection network (i.e. `primary-gie`) which can detect vehicles and people. We then pass that information on to a couple of different classification models (i.e. `secondary-gie[0,1,2]`) which are for classifying vehicle type, vehicle color, and vehicle make, respectively.\n",
    "\n",
    "Notice there is a also a `tracker` here (which is not enabled) that could be used for giving each instance of vehicle or person in the video it's own unique ID.  The tracker is able to be configured in an effort to speed up the pipeline by doing things like inferencing on one frame of the video stream and then running the tracker on a few subsequent frames instead of running the full detection model.\n",
    "\n",
    "Lastly, notice for each of the `gie` sections, we provide a `config-file`. Inside these config files, you can find all of the components which correspond to the model itself including model path, batch size, classes, custom parsers, etc.\n",
    "\n",
    "The image below depicts a similar pipeline to the one that we're building here (without the tracker and the message-broker which we'll touch later in the lab).\n",
    "\n",
    "<img src=\"images/ds_16.png\" style=\"width: 1080px; float: center;\">\n",
    "\n",
    "## Let's Try It\n",
    "\n",
    "Now that we understand a little bit about the DeepStream configuration files which will help build the pipeline, we can now see a few examples of what the output looks like. Note again that for this demonstration we will be creating an mp4 as output instead of on-screen display since we are inside of a Jupyter notebook, but the config files can be modified to use any of the `sink` options at a later time.\n",
    "\n",
    "First, let's copy our configuration files to the location where Deepstream was installed so that it's easier for it to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp configs/* /opt/nvidia/deepstream/deepstream-6.1/samples/configs/deepstream-app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the DeepStream SDK we can simply run `deepstream-app` with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepstream-app --help-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have put most of our configuration inside the config file we created earlier, we can simply use those instead of the command line arguments to successfully run `deepstream-app`.\n",
    "\n",
    "## 2 Streams\n",
    "\n",
    "Let's start with the 2 stream configuration that we discussed above (again, notice in the [configuration file](configs/source2_mp4.txt) that we are running the primary detector to detect vehicles and pedestrians as well as 3 secondary classifiers for car color, car make, and car model).\n",
    "\n",
    "You'll notice that when you first run the `deepstream-app` that there appears to be an error.  This can be overlooked since the error is simply pointing out that the TensorRT engine does not exist and that it needs to build it in order to continue.  Also, reading through some of the output will show you that INT8 is not supported by this platform (namely, Jetson Nano); so eventhough we set INT8 mode in our configuration file, the app will instead try to create the next-best thing which in this case is FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset DISPLAY && deepstream-app -c /opt/nvidia/deepstream/deepstream-6.1/samples/configs/deepstream-app/source2_mp4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video width=\"1280\" height=\"720\" controls>\n",
    "        <source src=\"out_2streams.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's experiment with adding a tracker. The below table shows a couple of the trackers that are part of the DeepStream SDK but you are also able to implement your own custom tracker. In order to visualize the tracker more easily, we set the number of input sources to 1.\n",
    "\n",
    "<img src=\"images/ds_12.png\" style=\"width: 1080px; float: center;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note we've set the number of sources to 1\n",
    "!sed -n 28,39p configs/source1_mp4_tracker.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how we add the tracker to our existing config file\n",
    "!sed -n 137,151p configs/source1_mp4_tracker.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unset DISPLAY && deepstream-app -c /opt/nvidia/deepstream/deepstream-6.1/samples/configs/deepstream-app/source1_mp4_tracker.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video width=\"1280\" height=\"720\" controls>\n",
    "        <source src=\"out_1streams_tracker.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Streams\n",
    "\n",
    "Now, let's try something a little bigger. Now let's try to remove the secondary classifiers but increase the number of streams we are processing at once. For this example, we will try 8 streams with the object detection model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unset DISPLAY && deepstream-app -c /opt/nvidia/deepstream/deepstream-6.1/samples/configs/deepstream-app/source8_mp4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice for this batch (images from that video stream), we are processing and running our detection model across all 8 videos streams at about 160 FPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video width=\"1280\" height=\"720\" controls>\n",
    "        <source src=\"out_8streams.mp4\" type=\"video/mp4\">\n",
    "    </video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "# (2) Building a Pipeline with the DeepStream Python API\n",
    "\n",
    "Now that we have gone through and created a few different pipelines with configuration files, let's look at the Python API for building a similar pipeline.\n",
    "\n",
    "Notice that the python API sits on top of the python API for GStreamer so should have most of the same functionality as you would get with the C/C++ API which sits on top of GStreamer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ds_11.png\" style=\"width: 1080px; float: center;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/nvidia/deepstream/deepstream-6.1/sources/deepstream_python_apps/apps/common')\n",
    "\n",
    "import gi\n",
    "gi.require_version('Gst', '1.0')\n",
    "from gi.repository import GObject, Gst\n",
    "\n",
    "from is_aarch_64 import is_aarch64\n",
    "from bus_call import bus_call\n",
    "\n",
    "import pyds\n",
    "\n",
    "# import helper function \n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaring class label ids and output video stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGIE_CLASS_ID_VEHICLE = 0\n",
    "PGIE_CLASS_ID_BICYCLE = 1\n",
    "PGIE_CLASS_ID_PERSON = 2\n",
    "PGIE_CLASS_ID_ROADSIGN = 3\n",
    "\n",
    "OUTPUT_VIDEO_NAME = \"out_2streams_python.mp4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy configuration file to deepstream_python_apps directory\n",
    "\n",
    "For this particular example, we'll be using [configs/dstest1_pgie_config.txt](configs/dstest1_pgie_config.txt).  Feel free check out the contents and try to match up what you see with the configuration file which we used in the first section.\n",
    "\n",
    "The cell below also highlights a portion of the configuration file that is important (i.e. defining the deep learning model we'll use, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -n 55,76p configs/dstest1_pgie_config.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp configs/dstest1_pgie_config.txt /opt/nvidia/deepstream/deepstream-6.1/sources/deepstream_python_apps/apps/deepstream-test1/dstest1_py.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing GStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GObject.threads_init()\n",
    "Gst.init(None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create GStreamer Pipeline to add elements to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipeline element that will form a connection of other elements\n",
    "print(\"Creating Pipeline \\n \")\n",
    "pipeline = Gst.Pipeline()\n",
    "\n",
    "if not pipeline:\n",
    "    sys.stderr.write(\" Unable to create Pipeline \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a source element for reading from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Source \\n \")\n",
    "source = Gst.ElementFactory.make(\"filesrc\", \"file-source\")\n",
    "if not source:\n",
    "    sys.stderr.write(\" Unable to create Source \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a h264 parser as the input file is an elementary h264 stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating H264Parser \\n\")\n",
    "h264parser = Gst.ElementFactory.make(\"h264parse\", \"h264-parser\")\n",
    "if not h264parser:\n",
    "    sys.stderr.write(\" Unable to create h264 parser \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using nvdec_h264 for accelerated decoding on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Decoder \\n\")\n",
    "decoder = Gst.ElementFactory.make(\"nvv4l2decoder\", \"nvv4l2-decoder\")\n",
    "if not decoder:\n",
    "    sys.stderr.write(\" Unable to create Nvv4l2 Decoder \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a nvstreammux instance to form batches for one or more sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Streaming Multiplexer\")\n",
    "streammux = Gst.ElementFactory.make(\"nvstreammux\", \"Stream-muxer\")\n",
    "if not streammux:\n",
    "    sys.stderr.write(\" Unable to create NvStreamMux \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting up nvinfer to run inference on the decoders output\n",
    "\n",
    "**Note:** Behavior of inference is set through the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Inference Plugin\")\n",
    "pgie = Gst.ElementFactory.make(\"nvinfer\", \"primary-inference\")\n",
    "if not pgie:\n",
    "    sys.stderr.write(\" Unable to create pgie \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using a converter to convert from NV12 to RGBA as required by nvosd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Video Converter (from NV12 to RGBA)\")\n",
    "nvvidconv = Gst.ElementFactory.make(\"nvvideoconvert\", \"convertor\")\n",
    "if not nvvidconv:\n",
    "    sys.stderr.write(\" Unable to create nvvideoconvert \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create OSD to draw on the converted RGBA buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating On-screen Display\")\n",
    "nvosd = Gst.ElementFactory.make(\"nvdsosd\", \"onscreendisplay\")\n",
    "if not nvosd:\n",
    "    sys.stderr.write(\" Unable to create nvosd \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create another converter for use with the output to mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Video Converter (from RGBA to I420)\")\n",
    "nvvidconv2 = Gst.ElementFactory.make(\"nvvideoconvert\", \"converter2\")\n",
    "if not nvvidconv2:\n",
    "    sys.stderr.write(\" Unable to create nvvideoconvert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create caps filter for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating capsfilter\")\n",
    "capsfilter = Gst.ElementFactory.make(\"capsfilter\", \"capsfilter\")\n",
    "if not capsfilter:\n",
    "    sys.stderr.write(\" Unable to create capsfilter\")\n",
    "    \n",
    "caps = Gst.Caps.from_string(\"video/x-raw, format=I420\")\n",
    "capsfilter.set_property(\"caps\", caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Encoder\")\n",
    "encoder = Gst.ElementFactory.make(\"avenc_mpeg4\", \"encoder\")\n",
    "if not encoder:\n",
    "    sys.stderr.write(\" Unable to create encoder\")\n",
    "    \n",
    "encoder.set_property(\"bitrate\", 2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create code parser for mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Code Parser\")\n",
    "codeparser = Gst.ElementFactory.make(\"mpeg4videoparse\", \"mpeg4-parser\")\n",
    "if not codeparser:\n",
    "    sys.stderr.write(\" Unable to create codeparser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create container for mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Container\")\n",
    "container = Gst.ElementFactory.make(\"qtmux\", \"qtmux\")\n",
    "if not container:\n",
    "    sys.stderr.write(\" Unable to create container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create filesink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Filesink\")\n",
    "sink = Gst.ElementFactory.make(\"filesink\", \"filesink\")\n",
    "if not sink:\n",
    "    sys.stderr.write(\" Unable to create sink\")\n",
    "\n",
    "sink.set_property(\"location\", OUTPUT_VIDEO_NAME)\n",
    "sink.set_property(\"sync\", 1)\n",
    "sink.set_property(\"async\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting PGIE and Streammux properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Playing file /opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_720p.h264 \")\n",
    "source.set_property('location', \\\n",
    "                    \"/opt/nvidia/deepstream/deepstream-6.1/samples/streams/sample_720p.h264\")\n",
    "streammux.set_property('width', 1920)\n",
    "streammux.set_property('height', 1080)\n",
    "streammux.set_property('batch-size', 1)\n",
    "streammux.set_property('batched-push-timeout', 4000000)\n",
    "pgie.set_property('config-file-path', \\\n",
    "                  \"/opt/nvidia/deepstream/deepstream-6.1/sources/deepstream_python_apps/apps/deepstream-test1/dstest1_py.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct pipeline\n",
    "\n",
    "Now we want to take all of the elements that we defined previously and build the pipeline which will be performing out video decoding, object detection, encoding, and writing out to an .mp4 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all elements to the pipeline\n",
    "pipeline.add(source)\n",
    "pipeline.add(h264parser)\n",
    "pipeline.add(decoder)\n",
    "pipeline.add(streammux)\n",
    "pipeline.add(pgie)\n",
    "pipeline.add(nvvidconv)\n",
    "pipeline.add(nvvidconv2)\n",
    "pipeline.add(encoder)\n",
    "pipeline.add(capsfilter)\n",
    "pipeline.add(codeparser)\n",
    "pipeline.add(container)\n",
    "pipeline.add(nvosd)\n",
    "pipeline.add(sink)\n",
    "\n",
    "# Start constructing the pipeline\n",
    "# Add the source and connect it to the h264 parser\n",
    "source.link(h264parser)\n",
    "# Connect the h264 parser output to the decoder\n",
    "h264parser.link(decoder)\n",
    "\n",
    "sinkpad = streammux.get_request_pad(\"sink_0\")\n",
    "if not sinkpad:\n",
    "    sys.stderr.write(\"Unable to get the sink pad of streammux \\n\")\n",
    "\n",
    "srcpad = decoder.get_static_pad(\"src\")\n",
    "if not srcpad:\n",
    "    sys.stderr.write(\"Unable to get source pad of decoder \\n\")\n",
    "\n",
    "# Connect the sink pad of the streammux to the src pad of the decoder\n",
    "srcpad.link(sinkpad)\n",
    "# Connect the PGIE (for inference)\n",
    "streammux.link(pgie)\n",
    "# Connect the Video Converter (from NV12 to RGBA)\n",
    "pgie.link(nvvidconv)\n",
    "# Connect OSD (to visualize the output/bounding boxes/etc.)\n",
    "nvvidconv.link(nvosd)\n",
    "# Connect the Video Converter (from RGBA to I420)\n",
    "nvosd.link(nvvidconv2)\n",
    "# Connect the Capsfilter\n",
    "nvvidconv2.link(capsfilter)\n",
    "# Connect encoder for mpeg4\n",
    "capsfilter.link(encoder)\n",
    "# Connect the code parser for mpeg4\n",
    "encoder.link(codeparser)\n",
    "\n",
    "sinkpad1 = container.get_request_pad(\"video_0\")\n",
    "if not sinkpad1:\n",
    "    sys.stderr.write(\"Unable to get the sink pad of qtmux \\n\")\n",
    "\n",
    "srcpad1 = codeparser.get_static_pad(\"src\")\n",
    "if not srcpad1:\n",
    "    sys.stderr.write(\"Unable to get mpeg4 parse src pad \\n\")\n",
    "\n",
    "# Connect sink pad from the container (i.e. the video output) to the source pad of the codeparser\n",
    "srcpad1.link(sinkpad1)\n",
    "# Create end of pipeline\n",
    "container.link(sink);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an event loop and feeding bus messages to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = GObject.MainLoop()\n",
    "#loop = GLib.MainLoop()\n",
    "bus = pipeline.get_bus()\n",
    "bus.add_signal_watch()\n",
    "bus.connect (\"message\", bus_call, loop);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a probe to get the meta data generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "osdsinkpad = nvosd.get_static_pad(\"sink\")\n",
    "if not osdsinkpad:\n",
    "    sys.stderr.write(\"Unable to get sink pad of nvosd \\n\")\n",
    "    \n",
    "osdsinkpad.add_probe(Gst.PadProbeType.BUFFER, osd_sink_pad_buffer_probe, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting pipeline \\n\")\n",
    "pipeline.set_state(Gst.State.PLAYING)\n",
    "try:\n",
    "    loop.run()\n",
    "except:\n",
    "    pass\n",
    "#cleanup\n",
    "pipeline.set_state(Gst.State.NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up inferencing interval:\n",
    "\n",
    "    0: inference every frame\n",
    "    1: inference every other frame (e.g. detect objects every other frame, use tracker to localize in-between)\n",
    "    2: inference every third frame\n",
    "   \n",
    "If desired, go to the [config file](configs/dstest1_pgie_config.txt) and change the interval as required.\n",
    "\n",
    "**NOTE:** We won't be covering this in this portion of the lab, but feel free to play around with it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have demonstrated the process of creating a DeepStream pipeline using both the configuration files as well as the Python API as well as demonstrated using a custom model with a custom video stream to do detection and traffic monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: left;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
